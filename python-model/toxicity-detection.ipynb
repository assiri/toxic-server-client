{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity Detection Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "import math\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('toxic_data_mid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd1ce3911c27f18f</td>\n",
       "      <td>your ga nomination of grey s anatomy  season ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04b92907a0db6e77</td>\n",
       "      <td>evan blass  update i have made the following c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9a70f2c3b9d16f5b</td>\n",
       "      <td>dear federico  when i have the time  and i p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25fe5bd99e7f4ffd</td>\n",
       "      <td>i don t think you people get it  metalcore is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bdfd40a86c88abe1</td>\n",
       "      <td>final point  i was planning on moving the tabl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  dd1ce3911c27f18f   your ga nomination of grey s anatomy  season ...   \n",
       "1  04b92907a0db6e77  evan blass  update i have made the following c...   \n",
       "2  9a70f2c3b9d16f5b    dear federico  when i have the time  and i p...   \n",
       "3  25fe5bd99e7f4ffd  i don t think you people get it  metalcore is ...   \n",
       "4  bdfd40a86c88abe1  final point  i was planning on moving the tabl...   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  toxic  \n",
       "0           0.0      0.0     0.0     0.0            0.0    0.0  \n",
       "1           0.0      0.0     0.0     0.0            0.0    0.0  \n",
       "2           0.0      0.0     0.0     0.0            0.0    0.0  \n",
       "3           0.0      0.0     0.0     0.0            0.0    0.0  \n",
       "4           0.0      0.0     0.0     0.0            0.0    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now']\n",
    "maxDictionaryLength = 8000\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(sentence, isCreateDict=False):\n",
    "    tmpTokens = sentence.lower().split()\n",
    "    tokens = [token for token in tmpTokens if ((token not in stopwords) and (len(token)> 0)) ]\n",
    "    #tokens = tmpTokens.filter((token) => !stopwords.includes(token) && token.length > 0);\n",
    "    \n",
    "    if isCreateDict:\n",
    "        for token in tokens:\n",
    "            if token in dictionary_dict:\n",
    "                dictionary_dict[token] += 1\n",
    "            else:\n",
    "                dictionary_dict[token] = 1\n",
    "    documentTokens.append(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def getInverseDocumentFrequency(documentTokens, dictionary):\n",
    "    return list(map(lambda word : 1 + math.log(len(documentTokens) / reduce(lambda acc,curr: (1 if (word in curr) else 0) + acc, documentTokens,0)),dictionary))\n",
    "\n",
    "\n",
    "  \n",
    "def encoder(sentence, dictionary, idfs):\n",
    "    tokens = tokenize(sentence)\n",
    "    tfs = getTermFrequency(tokens, dictionary)\n",
    "    tfidfs = getTfIdf(tfs,idfs)\n",
    "    return tfidfs\n",
    "\n",
    "\n",
    "def getTermFrequency(tokens, dictionary):\n",
    "    return  list(map(lambda token: reduce(lambda acc,curr : (acc + 1 if (curr == token) else acc), tokens,0), dictionary))\n",
    "\n",
    "\n",
    "\n",
    "def getTfIdf(tfs, idfs):\n",
    "    return [tf * idf for (tf,idf) in zip(tfs,idfs)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loved': 1, 'movie': 2, 'boring': 1}\n",
      "['movie', 'loved', 'boring']\n",
      "[1.0, 1.6931471805599454, 1.6931471805599454]\n",
      "[[1.0, 1.6931471805599454, 0.0], [1.0, 0.0, 1.6931471805599454]]\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Code used in the slides ( Module : preparing data for machine learning model )\n",
    "dictionary_dict = {}\n",
    "documentTokens = []\n",
    "testComments = ['i loved the movie', 'movie was boring']\n",
    "\n",
    "for comment in testComments:\n",
    "    documentTokens.append(tokenize(comment,True))\n",
    "\n",
    "\n",
    "dictionary = sorted(dictionary_dict, key=dictionary_dict.get, reverse=True)\n",
    "idfs = getInverseDocumentFrequency(documentTokens, dictionary);\n",
    "\n",
    "tfidfs = []\n",
    "\n",
    "for comment in testComments:\n",
    "    tfidfs.append(encoder(comment, dictionary, idfs))\n",
    "\n",
    "print(dictionary_dict)\n",
    "print(dictionary)\n",
    "print(idfs)\n",
    "print(tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_dict = {}\n",
    "documentTokens = []\n",
    "df['tokens'] = df['comment_text'].apply(lambda x : tokenize(x, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dd1ce3911c27f18f</td>\n",
       "      <td>your ga nomination of grey s anatomy  season ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[ga, nomination, grey, anatomy, season, articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04b92907a0db6e77</td>\n",
       "      <td>evan blass  update i have made the following c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[evan, blass, update, made, following, comment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9a70f2c3b9d16f5b</td>\n",
       "      <td>dear federico  when i have the time  and i p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[dear, federico, time, promise, ll, make, time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25fe5bd99e7f4ffd</td>\n",
       "      <td>i don t think you people get it  metalcore is ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[think, people, get, metalcore, genre, incorpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bdfd40a86c88abe1</td>\n",
       "      <td>final point  i was planning on moving the tabl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[final, point, planning, moving, tables, artic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  dd1ce3911c27f18f   your ga nomination of grey s anatomy  season ...   \n",
       "1  04b92907a0db6e77  evan blass  update i have made the following c...   \n",
       "2  9a70f2c3b9d16f5b    dear federico  when i have the time  and i p...   \n",
       "3  25fe5bd99e7f4ffd  i don t think you people get it  metalcore is ...   \n",
       "4  bdfd40a86c88abe1  final point  i was planning on moving the tabl...   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  toxic  \\\n",
       "0           0.0      0.0     0.0     0.0            0.0    0.0   \n",
       "1           0.0      0.0     0.0     0.0            0.0    0.0   \n",
       "2           0.0      0.0     0.0     0.0            0.0    0.0   \n",
       "3           0.0      0.0     0.0     0.0            0.0    0.0   \n",
       "4           0.0      0.0     0.0     0.0            0.0    0.0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [ga, nomination, grey, anatomy, season, articl...  \n",
       "1  [evan, blass, update, made, following, comment...  \n",
       "2  [dear, federico, time, promise, ll, make, time...  \n",
       "3  [think, people, get, metalcore, genre, incorpo...  \n",
       "4  [final, point, planning, moving, tables, artic...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary : 8000\n",
      "['fuck', 'article', 'u', 'bitch', 'es', 'page', 'wikipedia', 'talk', 'please', 'like']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dictionary = sorted(dictionary_dict, key=dictionary_dict.get, reverse=True)\n",
    "dictionary = dictionary[:maxDictionaryLength]\n",
    "print('Length of dictionary : {0}'.format(len(dictionary)))\n",
    "print(dictionary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs = getInverseDocumentFrequency(documentTokens, dictionary)\n",
    "len(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0, 10.12990748519189, 0.0, 0.0, 0.0, 0.0, 0...\n",
       "1    [0.0, 0.0, 0.0, 0.0, 0.0, 8.078458564119455, 0...\n",
       "2    [0.0, 0.0, 4.575550768806933, 0.0, 0.0, 0.0, 2...\n",
       "3    [0.0, 0.0, 0.0, 0.0, 0.0, 2.6928195213731514, ...\n",
       "4    [0.0, 2.5324768712979724, 0.0, 0.0, 0.0, 0.0, ...\n",
       "Name: features, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['features'] = df['comment_text'].apply(lambda x : encoder(x,dictionary, idfs))\n",
    "df['features'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df['features'].apply(lambda x : pd.Series(x))\n",
    "df_new['toxic'] = df['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630 train examples\n",
      "70 validation examples\n",
      "300 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df_new, test_size=0.3)\n",
    "train, val = train_test_split(train, test_size=0.1)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((630, 8001), (300, 8001), (70, 8001))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=16):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('toxic')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dataframe.values, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfFeatures = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation='relu', input_shape=(numOfFeatures,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.06),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 5)                 40005     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 40,011\n",
      "Trainable params: 40,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6986 - accuracy: 0.9143 - val_loss: 0.6771 - val_accuracy: 0.9429\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6746 - accuracy: 0.9540 - val_loss: 0.6801 - val_accuracy: 0.9429\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6722 - accuracy: 0.9603 - val_loss: 0.6854 - val_accuracy: 0.9286\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6700 - accuracy: 0.9635 - val_loss: 0.7015 - val_accuracy: 0.9000\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6667 - accuracy: 0.9714 - val_loss: 0.7154 - val_accuracy: 0.8857\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6651 - accuracy: 0.9746 - val_loss: 0.7181 - val_accuracy: 0.8714\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6721 - accuracy: 0.9635 - val_loss: 0.7319 - val_accuracy: 0.8571\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6656 - accuracy: 0.9778 - val_loss: 0.6970 - val_accuracy: 0.9143\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6649 - accuracy: 0.9778 - val_loss: 0.6832 - val_accuracy: 0.9429\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6838 - val_accuracy: 0.9286\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6628 - accuracy: 0.9825 - val_loss: 0.6847 - val_accuracy: 0.9286\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6855 - val_accuracy: 0.9286\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6857 - val_accuracy: 0.9286\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6858 - val_accuracy: 0.9286\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6858 - val_accuracy: 0.9286\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6858 - val_accuracy: 0.9286\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6861 - val_accuracy: 0.9286\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6862 - val_accuracy: 0.9286\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6862 - val_accuracy: 0.9286\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.9841 - val_loss: 0.6862 - val_accuracy: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe64693e7f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.summary()\n",
    "model.fit(train_ds,epochs=20 ,validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7014526724815369, 0.9100000262260437]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted probabliities : [[9.984712e-01]\n",
      " [4.631444e-20]]\n",
      "predicted classes : [[1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "## make predictions\n",
    "testComments = ['you suck', 'you are a great person']\n",
    "tfidfs = []\n",
    "for comment in testComments:\n",
    "    tfidfs.append(encoder(comment, dictionary, idfs))\n",
    "print(f'predicted probabliities : {model.predict(tfidfs)}')\n",
    "print(f'predicted classes : {tf.round(model.predict(tfidfs))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('toxicity_python.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tfjs_python_toxicity\n",
    "!mkdir tfjs_python_toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflowjs_converter --input_format=keras toxicity_python.h5 tfjs_python_toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dictionary and IDFs \n",
    "\n",
    "import json \n",
    "\n",
    "with open('tfjs_python_toxicity/dictionary.json', 'w') as outfile:\n",
    "    json.dump(dictionary, outfile)\n",
    "\n",
    "with open('tfjs_python_toxicity/idfs.json', 'w') as outfile:\n",
    "    json.dump(idfs, outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
